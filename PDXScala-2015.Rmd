# Introduction to Spark MLlib and machine learning

## Robert Dodier
## PDX Scala meetup

### MLlib

 * subproject of Apache Spark
     * other subprojects: Spark Core, Spark SQL, Spark Streaming, GraphX
 * basic statistical operations
 * classification and regression
 * clustering
 * dimensionality reduction
 * feature extraction and transformation
 * optimization functions
 * implemented in two packages
     * spark.mllib which is based on RDD's
     * spark.ml which is based on DataFrames (annotated RDD's)

### Resilient Distributed Dataset (RDD)
 
 * immutable
     * bulk operations transform one RDD into another
     * create initial RDD from a file or data in memory
 * distributed
     * partitions of records on nodes in a Spark cluster
 * fault-tolerant
     * recover from failures via lineage
     * recompute starting from most recent checkpoint
     * no cost if no failures

### Operations on RDD's

 * Transformations: map, filter, join, sample, union, ...
 * Actions: collect, reduce, count, save
 * Reminiscent of working with tables in SQL
     * to some degree also, vectors/matrices in R, Matlab, etc

### About machine learning ...

 * ML = statistical modeling applied to large, unstructured data sets
 * Classification, regression, clustering, latent variable models, ...
 * Training = estimating parameters (usually maximum likelihood)
 * What can you do with a ML model? We'll come back to that later 

### Opportunities for parallelism in ML

 * Applying same operation to many records
     * e.g. log likelihood = sum over records
 * partition available data into blocks, apply operation over blocks
     * e.g. cross validation = train and test on partitions
 * Monte Carlo methods
     * averaging over parameters, e.g. Markov chain MC
     * randomized search for parameter estimation
     * ensemble methods (model averaging)

### ML tasks and MLlib

 * Classification models
     * LogisticRegression, SVMModel, MultilayerPerceptronClassifier,
    NaiveBayesModel, RandomForestClassifier
 * Regression models
     * LinearRegressionModel, RandomForestRegressor, IsotonicRegression
 * Survival models
     * AFTSurvivalRegression
 * Clustering
     * LDAModel, GaussianMixtureModel, KMeansModel
 * Dimensionality reduction
     * SingularValueDecomposition, PCA, EigenValueDecomposition
 * Optimization
     * somethingWithSGD, LBFGS

### Classification

 * It's assumed that there are distinct groups
 * Some characteristics are observable
 * Given observations, figure out which group
 * When observables are continuous variables,
   this amounts to drawing curves to divide groups
 * Different classification models yield different curves

### Logistic regression and neural network classifiers

 * When each group has a Gaussian distribution,
   dividing curves are conic sections
 * If groups have the same covariance,
   curves are straight lines and class probability is
   $1/(1 + \exp(-z))$ where $z$ is a weighted combination
   of observables
 * Logistic regression is just that same-covariance model
 * Neural network (a.k.a. multilayer perceptron) is a number of
   logistic regression models lumped together
 * Each hidden unit is essentially a feature detector;
   final output is logistic regression model of features

### Classifiers applied to example problem

 * From Kaggle, Stackoverflow questions closed for various reasons
 * Training sample 140k records, containing all closed and equal number open
 * Condense all reasons into one, so target is closed/not closed
 * Construct data set containing five observables:
   owner age, reputation, owner undeleted answer count,
   owner closed post count, body length
 * Omit verbiage: title, body text, tags
     * perhaps use LDA to generate numerical scores for words

### An aside: Wilcoxon-Mann-Whitney and input relevance for classification

 * WMW statistic is a rank-sum statistic: $U = \sum_i R^+_i$ where $R^+_i$
   is the rank of the $i$'th positive example.
 * This is equivalent to the area under the ROC curve
   for a logistic regression model using just that one input
 * So WMW is a way to assess relevance without building a model

### WMW computed by operations on RDD's

```
  def U (XC: RDD[(Double, Int)]): Double =
  {
    val XC_sorted = XC.sortBy { case (a, b) => a }
    val foo = XC_sorted.zipWithIndex ()
    val bar = foo.map { case ((a, b), c) => (a, (b, c)) }
    val baz = bar.aggregateByKey ((0L, 0L)) ( {case ((a, b), (c, d)) => (a + 1, b + d)}, {case ((a, b), (c, d)) => (a + c, b + d)} )
    val quux = baz.map { case (a, (b, c)) => (a, c/b.toDouble) }
    val mumble = XC_sorted.join (quux)
    val blurf = mumble.filter { case (a, (b, c)) => b == 1 }
    val rank_sum = blurf.aggregate (0.0) ( {case (a, (b, (c, d))) => a + d}, {(a, b) => a + b} )
    val n = mumble.count ()
    val n1 = blurf.count ()
    val n0 = n - n1

    ((rank_sum + n1) - n1*(n1 + 1.0)/2.0)/(n1 * n0.toDouble)
  }
```

 * `foo`: append rank to sorted values
 * `bar`: make input value a key
 * `baz`, `quux`: compute average rank for tied values
 * `mumble`: assign average ranks
 * `blurf`: extract positive examples

### WMW applied to Stackoverflow data

 * Closer to 0 or 1 $\Rightarrow$ more relevance
 * Closer to 1 $\Rightarrow$ positive correlation
 * Closer to 0 $\Rightarrow$ negative correlation
 * Values:
     * 0.817: owner closed post count
     * 0.338: body length
     * 0.397: reputation
     * 0.402: owner undeleted answer count
     * 0.408: owner age
 
### Logistic regression for Stackoverflow data

Creating and training the model: (working in spark-shell here)

```
import org.apache.spark.mllib.util.MLUtils
val x = MLUtils.loadLibSVMFile (sc, "kaggle/stackoverflow-train-sample-replace-text-by-length-numeric.libsvm")
x.cache

import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
val lr = new LogisticRegressionWithLBFGS()
val model = lr.run (x)
```

Count true/false positive/negative: ("in sample", so this is optimistic)

```
val output_class = x.map (p => (model.predict (p.features), p.label))
val true_positives = output_class.filter (ab => ab._1 == 1.0 && ab._2 == 1.0).count
val false_positives = output_class.filter (ab => ab._1 == 1.0 && ab._2 == 0.0).count
val false_negatives = output_class.filter (ab => ab._1 == 0.0 && ab._2 == 1.0).count
val true_negatives = output_class.filter (ab => ab._1 == 0.0 && ab._2 == 0.0).count
```

Output:

```
scala> (true_positives, false_positives, false_negatives, true_negatives)
res2: (Long, Long, Long, Long) = (57596,14664,12540,55472)
```

Construct ROC curve:

```
model.clearThreshold
val output_score = x.map (p => (model.predict (p.features), p.label))
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics (output_score, 20)
val roc = metrics.roc.collect
metrics.areaUnderROC
```

Output:

```
scala> metrics.areaUnderROC
res4: Double = 0.8052311801579356
```

Plot ROC curve: (via Wisp)

```
import com.quantifind.charts.Highcharts._
line (roc)
title ("ROC")
xAxis ("False Positive rate")
yAxis ("True Positive rate")
```

### Neural network applied to Stackoverflow data

Load data and normalize each field to mean 0 and sd 1:

```
val x = MLUtils.loadLibSVMFile (sc, "kaggle/stackoverflow-train-sample-replace-text-by-length-numeric.libsvm")
val summary = Statistics.colStats (x.map {case LabeledPoint (l, f) => f})
val means = summary.mean.toArray
val sds = summary.variance.toArray.map {u => Math.sqrt (u)}
val y = x.map {case LabeledPoint (l, f) => LabeledPoint (l, Vectors.dense (((f.toArray zip means) zip sds).map {case ((u, v), w) => (u - v)/w}))}
```

Split data into training and testing sets:

```
val data = y.toDF
val splits = data.randomSplit(Array(0.5, 0.5), seed = 1L)
val train = splits(0)
val test = splits(1)
```

Create and train neural network model:

```
val layers = Array (5, 14, 2)
val mlpc = new MultilayerPerceptronClassifier ().setLayers (layers).setMaxIter (100)
val model = mlpc.fit (train)
```

Compute outputs for test data and evaluate:

```
val test_result = model.transform (test)
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
val predictionAndLabels = test_result.select ("prediction", "label")
val evaluator = new MulticlassClassificationEvaluator ()
evaluator.evaluate (test_result)
```

Output: (result is "precision" = (true positives)/(all positives))

```
scala> evaluator.evaluate (test_result)
res0: Double = 0.8473982981060162
```

Comparable number for logistic regression: `57596/(57596 + 14664) = 0.797`

### Prescriptive analytics: ML in a bigger picture

 * A rational decision maximizes expected utility
     * under assumptions about rational behavior
 * This is the "expected utility hypothesis" of decision theory
 * Expected utility = value of outcome averaged over possible outcomes
     * represent uncertainty as probability
     * represent value as utility
 * Purpose of ML is to compute probabilities
     * e.g. p(accept offer | personal data)
     * raises question of "calibration" of outputs
 * "Prescriptive analytics" = ML + decision theory
